# LLM Benchmarks (Assistant Usage Metrics)

This folder tracks **AI-assistant usage benchmarks** for this technical exercise:
- what docs were consulted (by file path)
- proxies for token cost (derived from chars read)

These metrics are **not product telemetry**. In a real system they would typically live outside the product
repository. For this exercise, we keep them **in-repo** so evaluators can inspect methodology and discipline.

## What is measured

We log runs as **append-only JSON Lines** in `runs.jsonl` (one run = one JSON object per line).

Each run includes:
- scenario id (see `SCENARIOS.md`)
- the list of docs consulted (`metrics.docs`)
- computed proxies:
  - `chars_read`: sum of character counts of the consulted docs
  - `tok_est`: `round(chars_read / 4)`
  - `max_doc_chars`: max characters among consulted docs

Important: metrics are validated by recomputing file sizes from `metrics.docs` to keep the data auditable.

## How to run (local)

1) Pick a scenario from `SCENARIOS.md` and run it as a **benchmark prompt** (include `#metrics`).
2) Ensure the assistant prints a final `METRICS ...` line (see `SCENARIOS.md`).
3) Append the run:
   - `python metrics/llm_benchmarks/scripts/add_run.py --from-stdin`
4) Validate:
   - `python metrics/llm_benchmarks/scripts/validate_runs.py`
5) Update summary:
   - `python metrics/llm_benchmarks/scripts/summarize.py --write metrics/llm_benchmarks/summary.md`

## Files

- `SCENARIOS.md`: benchmark scenarios + canonical prompt templates.
- `runs.jsonl`: append-only log of benchmark runs.
- `runs.sample.jsonl`: one sample run line (used for validation tooling).
- `runs.schema.json`: documentation-only JSON Schema for the run format.
- `summary.md`: human-readable summary (generated by `scripts/summarize.py`).
- `scripts/`: validation and helper utilities (stdlib-only).

